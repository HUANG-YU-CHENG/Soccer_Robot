"""
train_soccer.py - è¨“ç·´äººå½¢æ©Ÿå™¨äººè¸¢è¶³çƒ

ä½¿ç”¨ PPO æ¼”ç®—æ³•è¨“ç·´æ©Ÿå™¨äººï¼š
1. èµ°å‘è¶³çƒ
2. è¸¢çƒå‘çƒé–€

ä½¿ç”¨æ–¹å¼ï¼š
    python train_soccer.py
"""

import os
import torch
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecNormalize
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback
from stable_baselines3.common.monitor import Monitor

# å°å…¥è‡ªå®šç¾©ç’°å¢ƒ
from humanoid_soccer_env import HumanoidSoccerEnv


def make_env(rank, seed=0):
    """
    å»ºç«‹ç’°å¢ƒçš„å·¥å» å‡½æ•¸ï¼ˆç”¨æ–¼ä¸¦è¡Œç’°å¢ƒï¼‰
    """
    def _init():
        env = HumanoidSoccerEnv()
        env = Monitor(env)
        env.reset(seed=seed + rank)
        return env
    return _init


def train():
    """ä¸»è¨“ç·´å‡½æ•¸"""
    
    # ==================== è¨­å®š ====================
    
    # è¨“ç·´åƒæ•¸
    TOTAL_TIMESTEPS = 1_000_000     # ç¸½è¨“ç·´æ­¥æ•¸ï¼ˆå¯ä»¥èª¿æ•´ï¼‰
    N_ENVS = 8                      # ä¸¦è¡Œç’°å¢ƒæ•¸é‡ï¼ˆ4060 å»ºè­° 4-8ï¼‰
    SAVE_FREQ = 100_000             # æ¯å¤šå°‘æ­¥å­˜ä¸€æ¬¡ checkpoint
    EVAL_FREQ = 50_000              # æ¯å¤šå°‘æ­¥è©•ä¼°ä¸€æ¬¡
    
    # è³‡æ–™å¤¾è¨­å®š
    LOG_DIR = "./logs/soccer/"
    CHECKPOINT_DIR = "./checkpoints/soccer/"
    os.makedirs(LOG_DIR, exist_ok=True)
    os.makedirs(CHECKPOINT_DIR, exist_ok=True)
    
    # ç¢ºèª GPU
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è£ç½®: {device}")
    if device == "cuda":
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
    
    # ==================== å»ºç«‹ç’°å¢ƒ ====================
    
    print(f"\nğŸ“¦ å»ºç«‹ {N_ENVS} å€‹ä¸¦è¡Œç’°å¢ƒ...")
    
    # ä½¿ç”¨ SubprocVecEnv é€²è¡ŒçœŸæ­£çš„ä¸¦è¡Œï¼ˆæ¯” DummyVecEnv å¿«ï¼‰
    # å¦‚æœé‡åˆ°å•é¡Œï¼Œå¯ä»¥æ”¹ç”¨ DummyVecEnv
    env = SubprocVecEnv([make_env(i) for i in range(N_ENVS)])
    
    # æ­£è¦åŒ–è§€å¯Ÿå€¼å’Œçå‹µï¼ˆéå¸¸é‡è¦ï¼ï¼‰
    env = VecNormalize(
        env,
        norm_obs=True,
        norm_reward=True,
        clip_obs=10.0,
        clip_reward=10.0,
    )
    
    print(f"   è§€å¯Ÿç©ºé–“: {env.observation_space.shape}")
    print(f"   å‹•ä½œç©ºé–“: {env.action_space.shape}")
    
    # ==================== å»ºç«‹æ¨¡å‹ ====================
    
    print("\nğŸ§  å»ºç«‹ PPO æ¨¡å‹...")
    
    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        tensorboard_log=LOG_DIR,
        device=device,
        # PPO è¶…åƒæ•¸ï¼ˆå¯ä»¥èª¿æ•´ï¼‰
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,         # å¢åŠ æ¢ç´¢
        vf_coef=0.5,
        max_grad_norm=0.5,
        # ç¥ç¶“ç¶²è·¯è¨­å®š
        policy_kwargs={
            "net_arch": [dict(pi=[256, 256], vf=[256, 256])],
        },
    )
    
    print(f"   æ¨¡å‹åƒæ•¸é‡: {sum(p.numel() for p in model.policy.parameters()):,}")
    
    # ==================== è¨­å®š Callbacks ====================
    
    # Checkpoint callback - å®šæœŸå„²å­˜æ¨¡å‹
    checkpoint_callback = CheckpointCallback(
        save_freq=SAVE_FREQ // N_ENVS,
        save_path=CHECKPOINT_DIR,
        name_prefix="soccer_humanoid",
        save_replay_buffer=False,
        save_vecnormalize=True,
    )
    
    # å»ºç«‹è©•ä¼°ç’°å¢ƒ
    eval_env = DummyVecEnv([lambda: Monitor(HumanoidSoccerEnv())])
    eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False, training=False)
    
    # Eval callback - å®šæœŸè©•ä¼°ä¸¦å„²å­˜æœ€ä½³æ¨¡å‹
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=CHECKPOINT_DIR,
        log_path=LOG_DIR,
        eval_freq=EVAL_FREQ // N_ENVS,
        n_eval_episodes=5,
        deterministic=True,
    )
    
    # ==================== é–‹å§‹è¨“ç·´ ====================
    
    print(f"\nğŸš€ é–‹å§‹è¨“ç·´ {TOTAL_TIMESTEPS:,} æ­¥...")
    print(f"   TensorBoard: tensorboard --logdir={LOG_DIR}")
    print(f"   Checkpoints: {CHECKPOINT_DIR}")
    print("-" * 50)
    
    try:
        model.learn(
            total_timesteps=TOTAL_TIMESTEPS,
            callback=[checkpoint_callback, eval_callback],
            progress_bar=True,
        )
    except KeyboardInterrupt:
        print("\nâš ï¸  è¨“ç·´è¢«ä¸­æ–·ï¼Œæ­£åœ¨å„²å­˜æ¨¡å‹...")
    
    # ==================== å„²å­˜æœ€çµ‚æ¨¡å‹ ====================
    
    print("\nğŸ’¾ å„²å­˜æœ€çµ‚æ¨¡å‹...")
    model.save(os.path.join(CHECKPOINT_DIR, "soccer_humanoid_final"))
    env.save(os.path.join(CHECKPOINT_DIR, "vec_normalize.pkl"))
    
    print("âœ… è¨“ç·´å®Œæˆï¼")
    print(f"   æ¨¡å‹ä½ç½®: {CHECKPOINT_DIR}")
    
    env.close()
    eval_env.close()


if __name__ == "__main__":
    train()